{
  "cacheDate": "2026-02-28",
  "data": [
    {
      "id": "2602.23363",
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal"
      ],
      "abstract": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com",
      "description": "MediX-R1 introduces an **open-ended Reinforcement Learning (RL) framework** for **medical multimodal large language models (MLLMs)**. This framework enables the generation of **clinically grounded, free-form answers** beyond typical multiple-choice formats by fine-tuning a vision-language backbone with **Group Based RL**.",
      "mlTag": "RL",
      "appTag": "Healthcare",
      "arxivUrl": "https://arxiv.org/abs/2602.23363",
      "pdfUrl": "https://arxiv.org/pdf/2602.23363",
      "publishedAt": "2026-02-26T18:59:46Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23361",
      "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
      "authors": [
        "Sven Elflein",
        "Ruilong Li",
        "Sérgio Agostinho",
        "Zan Gojcic",
        "Laura Leal-Taixé",
        "Qunjie Zhou",
        "Aljosa Osep"
      ],
      "abstract": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.",
      "description": "This paper presents **VGG-T$^3$**, a scalable model for **offline feed-forward 3D reconstruction** that addresses computational and memory limitations. It overcomes the quadratic growth of requirements with respect to the number of **input images**, improving efficiency at scale.",
      "mlTag": "Vision",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23361",
      "pdfUrl": "https://arxiv.org/pdf/2602.23361",
      "publishedAt": "2026-02-26T18:59:33Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23360",
      "title": "Model Agreement via Anchoring",
      "authors": [
        "Eric Eaton",
        "Surbhi Goel",
        "Marcel Hussing",
        "Michael Kearns",
        "Aaron Roth",
        "Sikata Bela Sengupta",
        "Jessica Sorrell"
      ],
      "abstract": "Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.\n  We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.",
      "description": "This work explores controlling **model disagreement**, defined as the extent to which predictions from **two machine learning models** differ. It adopts a standard notion of disagreement using the **expected squared difference** in real-valued predictions, aiming to understand and manage this discrepancy.",
      "mlTag": "Theory",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23360",
      "pdfUrl": "https://arxiv.org/pdf/2602.23360",
      "publishedAt": "2026-02-26T18:59:32Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23359",
      "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation",
      "authors": [
        "Vaibhav Agrawal",
        "Rishubh Parihar",
        "Pradhaan Bhat",
        "Ravi Kiran Sarvadevabhatla",
        "R. Venkatesh Babu"
      ],
      "abstract": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.",
      "description": "SeeThrough3D identifies **occlusion reasoning** as a critical, overlooked aspect of **3D layout-conditioned text-to-image generation**. The method focuses on synthesizing **partially occluded objects** with **depth-consistent geometry and scale**, improving realism in generated scenes.",
      "mlTag": "Generative",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23359",
      "pdfUrl": "https://arxiv.org/pdf/2602.23359",
      "publishedAt": "2026-02-26T18:59:05Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23358",
      "title": "A Dataset is Worth 1 MB",
      "authors": [
        "Elad Kimchi Shoshani",
        "Leeyam Gabay",
        "Yedid Hoshen"
      ],
      "abstract": "A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.",
      "description": "This paper addresses the significant **communication costs** incurred when a **dataset server** distributes large payloads to many clients. It highlights that transmitting **raw data** for client-side training is often necessary due to diverse hardware and software, unlike pre-trained models.",
      "mlTag": "Data",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23358",
      "pdfUrl": "https://arxiv.org/pdf/2602.23358",
      "publishedAt": "2026-02-26T18:59:03Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23357",
      "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training",
      "authors": [
        "Aheli Saha",
        "René Schuster",
        "Didier Stricker"
      ],
      "abstract": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.",
      "description": "This research investigates **sensor generalization** for **adaptive sensing** in **event-based object detection** using bio-inspired event cameras. These cameras offer **asynchronous, low-latency, and high dynamic range** capabilities, posing unique challenges due to the novel nature of their output signals.",
      "mlTag": "Vision",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23357",
      "pdfUrl": "https://arxiv.org/pdf/2602.23357",
      "publishedAt": "2026-02-26T18:57:52Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23353",
      "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
      "authors": [
        "Simon Roschmann",
        "Paul Krzakala",
        "Sonia Mazelet",
        "Quentin Bouniot",
        "Zeynep Akata"
      ],
      "abstract": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.",
      "description": "SOTAlign proposes a **semi-supervised method** for **aligning frozen pretrained unimodal vision and language models**. Utilizing **optimal transport**, it aims to achieve a shared statistical model of the world, building on the **Platonic Representation Hypothesis**.",
      "mlTag": "Other",
      "appTag": "NLP",
      "arxivUrl": "https://arxiv.org/abs/2602.23353",
      "pdfUrl": "https://arxiv.org/pdf/2602.23353",
      "publishedAt": "2026-02-26T18:55:06Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23351",
      "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning",
      "authors": [
        "Amita Kamath",
        "Jack Hessel",
        "Khyathi Chandu",
        "Jena D. Hwang",
        "Kai-Wei Chang",
        "Ranjay Krishna"
      ],
      "abstract": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.",
      "description": "This paper posits that the **lack of reasoning capabilities** in **Vision-Language Models (VLMs)** stems from **reporting bias** in their training data. It argues that how people communicate about visual content often omits **tacit information** crucial for robust reasoning.",
      "mlTag": "LLMs",
      "appTag": "NLP",
      "arxivUrl": "https://arxiv.org/abs/2602.23351",
      "pdfUrl": "https://arxiv.org/pdf/2602.23351",
      "publishedAt": "2026-02-26T18:54:06Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23349",
      "title": "FlashOptim: Optimizers for Memory Efficient Training",
      "authors": [
        "Jose Javier Gonzalez Ortiz",
        "Abhay Gupta",
        "Chris Renard",
        "Davis Blalock"
      ],
      "abstract": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.\n  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.",
      "description": "FlashOptim introduces a family of **optimizers** designed for **memory-efficient training** of neural networks. It addresses the substantial **accelerator memory** footprint required for model parameters, gradients, and **optimizer state variables** during standard **mixed-precision training**.",
      "mlTag": "Optimization",
      "appTag": "General",
      "arxivUrl": "https://arxiv.org/abs/2602.23349",
      "pdfUrl": "https://arxiv.org/pdf/2602.23349",
      "publishedAt": "2026-02-26T18:52:22Z",
      "mediaUrls": [],
      "source": "arxiv"
    },
    {
      "id": "2602.23342",
      "title": "AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search",
      "authors": [
        "Weijian Chen",
        "Haotian Liu",
        "Yangshen Deng",
        "Long Xiang",
        "Liang Huang",
        "Gezi Li",
        "Bo Tang"
      ],
      "abstract": "On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.\n  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.",
      "description": "AlayaLaser presents an efficient approach for **on-disk graph-based approximate nearest neighbor search (ANNS)**, critical for **large-scale, high-dimensional vector similarity search**. It tackles **prohibitive I/O costs** through an optimized **index layout and search strategy**, improving system performance.",
      "mlTag": "Systems",
      "appTag": "Retrieval",
      "arxivUrl": "https://arxiv.org/abs/2602.23342",
      "pdfUrl": "https://arxiv.org/pdf/2602.23342",
      "publishedAt": "2026-02-26T18:48:29Z",
      "mediaUrls": [],
      "source": "arxiv"
    }
  ]
}